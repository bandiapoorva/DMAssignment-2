# DMAssignment-2 
1. This project focuses on implementing a Naive Bayes Classifier (NBC) to distinguish between essays written by humans and those generated by Language Learning Models (LLMs). Using a dataset comprising both types of essays, the project involves developing a classifier based on Bayesâ€™ theorem, which assumes feature independence for text classification. Key steps include data preprocessing, vocabulary creation, probability assessment with smoothing, and evaluating classifier performance. The goal is to accurately classify essays into human-written or LLM-generated categories, showcasing the practical application of NBC in text classification.
2. The dataset used in this project comprises two types of essays: human-written and those generated by a Language Learning Model (LLM), specifically ChatGPT.
3. analyze the distribution of essay lengths to understand the textual characteristics of the two classes of essays in our dataset.
4. we partition our combined dataset into training and development sets. Utilizing a 90-10 split, we allocate 90% of the data for training our Naive Bayes Classifier, while the remaining 10% will serve as the development set to validate the model's performance.
5. To quantify the likelihood of each word's occurrence and its association with either class, we calculate probabilities using a smoothed Naive Bayes approach. For each word in our lexicon, we determine its frequency within human-written and LLM-generated essays, applying Laplace smoothing with an alpha of 1.0 to avoid zero probabilities.
6. In the classification stage, I applied the Naive Bayes Classifier to the development dataset, utilizing log probabilities to mitigate underflow issues. The model achieved an accuracy of 71%, indicating that it was able to correctly identify the human or LLM written essays with a substantial degree of success. This performance reflects the effectiveness of the feature extraction and probability calculation methods employed in earlier steps.
7. I conducted a smoothing parameter analysis to optimize the Naive Bayes Classifier, testing alpha values of 0.5, 1, 1.5, and 2. The consistent accuracy of 71% across all alpha levels suggests that the model's performance is robust to the choice of the smoothing parameter within the tested range.
8. Identifying the most predictive words for each class, I found that certain words like 'carolina' and 'nobody' are more indicative of human authorship, while words such as 'progressive' and 'campain' are more characteristic of LLM-generated texts.
9. The smoothing analysis performed on the development set revealed a consistent classifier accuracy of 71% across various alpha values, indicating the model's stability against the smoothing parameter.
